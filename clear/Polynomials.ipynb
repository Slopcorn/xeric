{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing polynomials with strange ways\n",
    "\n",
    "We'll be minimizing single and multivariable quartic functions (without using the knowledge it is one) in this file to see if we can make this stuff actually work. This leads up to attempting to use these methods on a machine learning model in the other file (ScikitModels.ipynb).\n",
    "\n",
    "Author: Raido Everest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize  # checking goodness of result compared to scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate functions to test on\n",
    "def quartic():\n",
    "    # default ranges for variables too\n",
    "    a, b = random.random() * 4.9 + 0.1, random.random() * 10 - 5\n",
    "    c, d = random.random() * 40 - 20, random.random() * 500 - 250\n",
    "    \n",
    "    return lambda x: a * x**4 + b * x**3 + c * x**2 + d * x\n",
    "\n",
    "# Making an nd-quartic function to test higher dimensionalities.\n",
    "# For the sake of generality, this should be used for the single variable case as well.\n",
    "# n - how many inputs the function takes\n",
    "# outputs the sum of n random quartic functions\n",
    "def quartic_n(n):\n",
    "    fs = [quartic() for _ in range(n)]\n",
    "    return lambda answers: sum(map(lambda pair: pair[0](pair[1]), zip(fs, answers)))\n",
    "\n",
    "# Example scipy optimization of a 4d quartic function:\n",
    "# minimize(quartic_n(4), (0,0,0,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differential Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main function that returns the best set of inputs it finds.\n",
    "# f - the function.\n",
    "# n - input dimensionality\n",
    "# k - population size\n",
    "# scaling - the scaling parameter when creating a new input set\n",
    "# loops   - how many loops it will do before giving up finding a better solution\n",
    "# Outputs the minimum value of the function that it found and the necessary input for it\n",
    "def diff_evo(f, n, k = 80, scaling = 0.5, loops=25):\n",
    "    # Create initial input set\n",
    "    pop       = create_population(n, k)\n",
    "    fitnesses = calculate_fitness(f, pop)\n",
    "    best, bestval = getbest(pop, fitnesses)   # pair of best input and its value\n",
    "    \n",
    "    loops_since_improvement = 0  # Keep it going until it's not working anymore.\n",
    "    while loops_since_improvement < loops:\n",
    "        loops_since_improvement += 1\n",
    "        \n",
    "        # Create next population by mutating previous one\n",
    "        newpop       = create_next_population(pop, scaling)\n",
    "        newfitnesses = calculate_fitness(f, newpop)\n",
    "        nextbest, nextbestval = getbest(newpop, newfitnesses)\n",
    "        \n",
    "        # Keep track of what the best outcome is\n",
    "        if nextbestval < bestval:\n",
    "            best, bestval = nextbest, nextbestval\n",
    "            loops_since_improvement = 0\n",
    "        \n",
    "        # Always choose the better one of the two choices to represent the 'next generation'\n",
    "        for i in range(k):\n",
    "            if newfitnesses[i] < fitnesses[i]:  # if something must be changed\n",
    "                pop[i], fitnesses[i] = newpop[i], newfitnesses[i]\n",
    "    \n",
    "    # Return the best value and its inputs\n",
    "    return bestval, best\n",
    "\n",
    "# Creates a population not knowing any previous information\n",
    "# n - dimensionality of output\n",
    "# k - population size\n",
    "def create_population(n, k):\n",
    "    # Arbitrary range, but should be fine for the time being.\n",
    "    return [[random.random() * 200 - 100 for _ in range(n)] for _ in range(k)]\n",
    "\n",
    "# Creates the next generation of input sets\n",
    "# pop - the old population\n",
    "# scaling - the scaling parameter\n",
    "def create_next_population(pop, scaling = 0.5):\n",
    "    dim = len(pop[0])\n",
    "    n = len(pop)\n",
    "    \n",
    "    newpop = [None] * n\n",
    "    for i in range(n):\n",
    "        a, b = random.randint(0, n-1), random.randint(0, n-1)  # Indices of two random elements\n",
    "        diff = [(pop[a][d] - pop[b][d]) for d in range(dim)]   # Difference of two random input vectors\n",
    "        newpop[i] = [pop[i][d] + diff[d] * scaling for d in range(dim)]  # Mutated input has been created\n",
    "    \n",
    "    return newpop\n",
    "\n",
    "# Just makes a list of evaluation results\n",
    "def calculate_fitness(f, pop):\n",
    "    return [f(inputs) for inputs in pop]\n",
    "\n",
    "# Given a population and fitnesses, returns the best element and its fitness.\n",
    "def getbest(pop, fitnesses):\n",
    "    best, bestfitness = pop[0], fitnesses[0]\n",
    "    for i in range(1, len(fitnesses)):\n",
    "        if fitnesses[i] < bestfitness:\n",
    "            best, bestfitness = pop[i], fitnesses[i]\n",
    "    return best, bestfitness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particle Swarm Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a super dodgy PSO that resets move speed to some fixed baseline at random times.\n",
    "# f - the function.\n",
    "# n - input dimensionality\n",
    "# k - population size\n",
    "# loops   - how many loops it will do before giving up finding a better solution\n",
    "# lr - how much velocity affects future velocity (between 0 and 1)\n",
    "# c1 and c2 - weighting for personal and overall best when moving  (between 0 and 1)\n",
    "# res_speed - the speed we may reset to randomly\n",
    "# res_speed_p - probability of reset\n",
    "# Outputs the minimum value of the function that it found and the necessary input for it\n",
    "def pso(f, n, k=25, loops=25, lr=1.0, c1=0.5, c2=0.5, res_speed=10, res_speed_p=0.1):\n",
    "    # Create initial population - including velocity, personal best locations\n",
    "    pop, velocity, pb_locs = create_pso(n, k)\n",
    "    # Also calculate personal best actual values.\n",
    "    pb_vals = calculate_fitness(f, pb_locs)\n",
    "    vals    = pb_vals[::]\n",
    "    g_best_loc, g_best_val = getbest(pop, vals)\n",
    "    \n",
    "    loops_since_improvement = 0\n",
    "    while loops_since_improvement < loops:\n",
    "        loops_since_improvement += 1\n",
    "        # Create the next generation - updates population, velocity\n",
    "        iterate_pso(pop, velocity, pb_locs, g_best_loc, lr, c1, c2, res_speed, res_speed_p)\n",
    "        # Now update values, personal bests, global bests\n",
    "        vals = calculate_fitness(f, pop)\n",
    "        update_personal_best(pb_vals, pb_locs, vals, pop)\n",
    "        next_best_loc, next_best_val = getbest(pop, vals)\n",
    "        if next_best_val < g_best_val:\n",
    "            loops_since_improvement = 0\n",
    "            g_best_loc, g_best_val = next_best_loc, next_best_val\n",
    "        \n",
    "    return g_best_val, g_best_loc  # best output and input\n",
    "\n",
    "# n - input dimensionality\n",
    "# k - population size\n",
    "def create_pso(n, k):\n",
    "    pop = create_population(n, k)  # Just use the same population init as DE\n",
    "    velocity = norm(create_population(n, k), 10)\n",
    "    pb_locs  = pop[::]\n",
    "    return pop, velocity, pb_locs\n",
    "\n",
    "# Iterates the PSO state.\n",
    "# pop - current locations\n",
    "# velocity - how fast we are moving and where\n",
    "# pb_locs - the best positions value wise each element has been to\n",
    "# best_loc - globally the best position that everybody also wants to move toward\n",
    "# lr - how much velocity affects future velocity (between 0 and 1)\n",
    "# c1 and c2 - weighting for personal and overall best when moving  (between 0 and 1)\n",
    "# res_speed - the speed we may reset to randomly\n",
    "# res_speed_p - probability of reset\n",
    "def iterate_pso(pop, velocity, pb_locs, best_loc, lr, c1, c2, res_speed, speed_res_p):\n",
    "    for i in range(len(pop)):\n",
    "        z1, z2 = random.random(), random.random()\n",
    "        velocity[i] = list(np.add(lr * np.array(velocity[i]),\n",
    "                                  np.add(c1*z1*np.subtract(pb_locs[i], pop[i]), c2*z2*np.subtract(best_loc, pop[i]))))\n",
    "        pop[i] = list(np.add(pop[i], velocity[i]))\n",
    "    \n",
    "    if random.random() < speed_res_p:\n",
    "        norm(velocity, res_speed)  # I will basically reset the speed my swarm moves at randomly - seems to help...\n",
    "    \n",
    "# does what it says\n",
    "def update_personal_best(pb_vals, pb_locs, vals, locs):\n",
    "    for i in range(len(vals)):\n",
    "        if vals[i] < pb_vals[i]:\n",
    "            pb_vals[i], pb_locs[i] = vals[i], locs[i]\n",
    "    \n",
    "# updates a list of vectors in place such that they get a certain length\n",
    "# vectors - list of vectors that all have the same dimensions\n",
    "def norm(vectors, tolength=1):\n",
    "    if len(vectors) == 0:\n",
    "        return vectors\n",
    "    dim = len(vectors[0])\n",
    "    # length of vector is the sqrt of its dot product with itself\n",
    "    # simply divide each component with that value\n",
    "    for i in range(len(vectors)):\n",
    "        length = np.dot(vectors[i], vectors[i])**0.5\n",
    "        for j in range(dim):\n",
    "            vectors[i][j] /= length * tolength\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does as the others do.\n",
    "# f - the function.\n",
    "# n - input dimensionality\n",
    "# k - population size\n",
    "# loops   - how many loops it will do before giving up finding a better solution\n",
    "def ga(f, n, k=100, loops=25):\n",
    "    # Create initial input set\n",
    "    pop       = create_population(n, k)\n",
    "    fitnesses = calculate_fitness(f, pop)\n",
    "    sorted(list(zip(pop, fitnesses)))\n",
    "    best, bestval = getbest(pop, fitnesses)   # pair of best input and its value\n",
    "    \n",
    "    loops_since_improvement = 0  # Keep it going until it's not working anymore.\n",
    "    while loops_since_improvement < loops:\n",
    "        loops_since_improvement += 1\n",
    "        \n",
    "        better_half = list(map(lambda x: x[0], sorted(list(zip(pop, fitnesses)), key=lambda x:x[1])[:k//2]))\n",
    "        next_half   = crossover(better_half)\n",
    "        mutate(next_half)  # chaotic\n",
    "        \n",
    "        pop = better_half + next_half\n",
    "        fitnesses = calculate_fitness(f, pop)\n",
    "        nextbest, nextbestval = getbest(pop, fitnesses)\n",
    "        \n",
    "        if nextbestval < bestval:\n",
    "            best, bestval = nextbest, nextbestval\n",
    "            loops_since_improvement = 0\n",
    "    \n",
    "    # Return the best value and its inputs\n",
    "    return bestval, best\n",
    "\n",
    "# random crossover between the better elements\n",
    "def crossover(inputs):\n",
    "    next_inputs = []\n",
    "    for _ in range(len(inputs)):\n",
    "        next_input = []\n",
    "        a, b = random.choice(inputs), random.choice(inputs)\n",
    "        for i in range(len(a)):  # for each dimension of input\n",
    "            next_input.append(random.choice([a[i], b[i]]))\n",
    "        next_inputs.append(next_input)\n",
    "    return next_inputs\n",
    "\n",
    "# performs mutation\n",
    "def mutate(inputs):\n",
    "    for x in inputs:\n",
    "        if random.random() < 0.5:  # let some of them be\n",
    "            for i in range(len(x)):  # the mutation here is scaling, s'all\n",
    "                x[i] *= random.random() * 1.5 + 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Goodness of Differential Evolution\n",
    "\n",
    "I expect scipy and diff_evo to be pretty close overall by quality of result. Turns out, they are, at least in this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a bunch of functions. Use scipy and diff_evo to find optimal solutions.\n",
    "# Arbitrarily choose amount of functions for each dimensionality.\n",
    "functions = 10\n",
    "\n",
    "print('Measuring difference of scipy and diff_evo - the higher, the better for DE')\n",
    "for dimensions in range(1,5):\n",
    "    print(f'Testing out {dimensions} dimensions...')\n",
    "    for _ in range(functions):\n",
    "        function = quartic_n(dimensions)\n",
    "\n",
    "        sp_ans = minimize(function, [0] * dimensions)\n",
    "        sp_bestval, sp_bestinput = sp_ans.fun, sp_ans.x\n",
    "\n",
    "        bestval, bestinput = diff_evo(function, dimensions)\n",
    "\n",
    "        print(f'Difference between scipy and diffevo: {sp_bestval - bestval}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Goodness of Particle Swarm Optimization\n",
    "\n",
    "It's not bad. Sometimes gets a really good result in the end, thousands below the scipy default optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = 10\n",
    "\n",
    "print('Measuring difference of scipy and PSO - the higher, the better for PSO')\n",
    "for dimensions in range(1,5):\n",
    "    print(f'Testing out {dimensions} dimensions...')\n",
    "    for _ in range(functions):\n",
    "        function = quartic_n(dimensions)\n",
    "\n",
    "        sp_ans = minimize(function, [0] * dimensions)\n",
    "        sp_bestval, sp_bestinput = sp_ans.fun, sp_ans.x\n",
    "\n",
    "        bestval, bestinput = pso(function, dimensions)\n",
    "\n",
    "        print(f'Difference between scipy and PSO: {sp_bestval - bestval}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Goodness of Genetic Algorithm\n",
    "\n",
    "Surprisingly enough, it actually works. The default parameters might not be the best, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = 10\n",
    "\n",
    "print('Measuring difference of scipy and GA - the higher, the better for GA')\n",
    "for dimensions in range(1,5):\n",
    "    print(f'Testing out {dimensions} dimensions...')\n",
    "    for _ in range(functions):\n",
    "        function = quartic_n(dimensions)\n",
    "\n",
    "        sp_ans = minimize(function, [0] * dimensions)\n",
    "        sp_bestval, sp_bestinput = sp_ans.fun, sp_ans.x\n",
    "\n",
    "        bestval, bestinput = ga(function, dimensions)\n",
    "\n",
    "        print(f'Difference between scipy and GA: {sp_bestval - bestval}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding good parameters\n",
    "\n",
    "I'll be trying to find the fastest set of parameters for these methods such that they still get decent results most of the time. Say, better or within a few percentage points at least 90% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll be using this function to generate tests. Parameters are all\n",
    "# but the first two (function and dimensionality)\n",
    "def test(f, parameters):\n",
    "    suitable = 0\n",
    "    n = 100\n",
    "    target = 90\n",
    "    dim = 4\n",
    "    \n",
    "    full_params = [None, dim] + parameters\n",
    "    for _ in range(n):\n",
    "        q = quartic_n(dim)\n",
    "        full_params[0] = q\n",
    "        sp_ans = minimize(q, np.zeros(dim)).fun\n",
    "        f_ans  = f(*full_params)[0]\n",
    "        # Better answer or very little absolute difference or small relative difference\n",
    "        abs_diff = abs(f_ans - sp_ans)\n",
    "        rel_diff = abs(f_ans) / abs(sp_ans)\n",
    "        if f_ans < sp_ans or abs_diff < 1 or rel_diff > 0.95 and rel_diff < 1.05:\n",
    "            suitable += 1\n",
    "    \n",
    "    return suitable, suitable >= target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some values with trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "test(diff_evo, [30, 0.25, 25])  # Smaller jumps seem to help a lot and let us cut population quite a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "#test(pso, [10, 20, 1.0, 0.75, 0.75, 10, 0.2])   # Speed reset strangely helps a lot with bad params\n",
    "#test(pso, [10, 20, 0.75, 0.75, 0.75, 10, 0.0])  # Without speed reset but decent learning rate picks, takes way longer\n",
    "test(pso, [14, 20, 0.8, 0.75, 0.75, 10, 0.1])    # Doing both, seems okay, but doesn't help lower population anyway\n",
    "\n",
    "# Increasing population seems to actually help converge faster - at least here, where fitness calculations are cheap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "test(ga, [400, 25])\n",
    "\n",
    "# The usual GA doesn't do the best job, but its performance could be changed by so many\n",
    "# details in the implementation (how many children do you make, how many of the best you pick, \n",
    "#                                how you do crossover and mutation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing the methods\n",
    "\n",
    "As one might expect, these may not be the greatest methods to use for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -r 1 -n 1 test(diff_evo, [30, 0.25, 25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -r 1 -n 1 test(pso, [14, 20, 0.8, 0.75, 0.75, 10, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -r 1 -n 1 test(ga, [400, 25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for fun, see how much of this time scipy minimization would take\n",
    "%timeit -r 1 -n 1 [minimize(quartic_n(4), (0,0,0,0)) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs\n",
    "\n",
    "I'll now modify the main driving functions to have them return not the best thing it found in the end, but history of best fitnesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_evo(f, n, k = 80, scaling = 0.5, loops=25):\n",
    "    # Create initial input set\n",
    "    pop       = create_population(n, k)\n",
    "    fitnesses = calculate_fitness(f, pop)\n",
    "    best, bestval = getbest(pop, fitnesses)   # pair of best input and its value\n",
    "    his = [bestval]\n",
    "    \n",
    "    loops_since_improvement = 0  # Keep it going until it's not working anymore.\n",
    "    while loops_since_improvement < loops:\n",
    "        loops_since_improvement += 1\n",
    "        \n",
    "        newpop       = create_next_population(pop, scaling)\n",
    "        newfitnesses = calculate_fitness(f, newpop)\n",
    "        nextbest, nextbestval = getbest(newpop, newfitnesses)\n",
    "        \n",
    "        if nextbestval < bestval:\n",
    "            best, bestval = nextbest, nextbestval\n",
    "            loops_since_improvement = 0\n",
    "        \n",
    "        for i in range(k):\n",
    "            if newfitnesses[i] < fitnesses[i]:  # if something must be changed\n",
    "                pop[i], fitnesses[i] = newpop[i], newfitnesses[i]\n",
    "        his.append(bestval)\n",
    "    \n",
    "    return his\n",
    "\n",
    "def pso(f, n, k=25, loops=25, lr=1.0, c1=0.5, c2=0.5, res_speed=10, res_speed_p=0.1):\n",
    "    pop, velocity, pb_locs = create_pso(n, k)\n",
    "    pb_vals = calculate_fitness(f, pb_locs)\n",
    "    vals    = pb_vals[::]\n",
    "    g_best_loc, g_best_val = getbest(pop, vals)\n",
    "    his = [g_best_val]\n",
    "    \n",
    "    loops_since_improvement = 0\n",
    "    while loops_since_improvement < loops:\n",
    "        loops_since_improvement += 1\n",
    "        iterate_pso(pop, velocity, pb_locs, g_best_loc, lr, c1, c2, res_speed, res_speed_p)\n",
    "        vals = calculate_fitness(f, pop)\n",
    "        update_personal_best(pb_vals, pb_locs, vals, pop)\n",
    "        next_best_loc, next_best_val = getbest(pop, vals)\n",
    "        if next_best_val < g_best_val:\n",
    "            loops_since_improvement = 0\n",
    "            g_best_loc, g_best_val = next_best_loc, next_best_val\n",
    "        his.append(g_best_val)\n",
    "        \n",
    "    return his\n",
    "\n",
    "def ga(f, n, k=100, loops=25):\n",
    "    pop       = create_population(n, k)\n",
    "    fitnesses = calculate_fitness(f, pop)\n",
    "    sorted(list(zip(pop, fitnesses)))\n",
    "    best, bestval = getbest(pop, fitnesses)\n",
    "    his = [bestval]\n",
    "    \n",
    "    loops_since_improvement = 0\n",
    "    while loops_since_improvement < loops:\n",
    "        loops_since_improvement += 1\n",
    "        \n",
    "        better_half = list(map(lambda x: x[0], sorted(list(zip(pop, fitnesses)), key=lambda x:x[1])[:k//2]))\n",
    "        next_half   = crossover(better_half)\n",
    "        mutate(next_half)\n",
    "        \n",
    "        pop = better_half + next_half\n",
    "        fitnesses = calculate_fitness(f, pop)\n",
    "        nextbest, nextbestval = getbest(pop, fitnesses)\n",
    "        \n",
    "        if nextbestval < bestval:\n",
    "            best, bestval = nextbest, nextbestval\n",
    "            loops_since_improvement = 0\n",
    "        his.append(bestval)\n",
    "    \n",
    "    return his"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we may plot these very easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function = quartic_n(4)\n",
    "\n",
    "de_his = diff_evo(function, 4, 30, 0.25, 25)\n",
    "ps_his = pso(function, 4, 14, 20, 0.8, 0.75, 0.75, 10, 0.1)\n",
    "ga_his = ga(function, 4, 400, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(data, method):\n",
    "    sns.lineplot(data = data)\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Best value')\n",
    "    plt.title(f'History of best value by generation using {method}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(de_his, 'Differential Evolution')\n",
    "plot(ps_his, 'Particle Swarm')\n",
    "plot(ga_his, 'Genetic Algorithm (generic?)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps it would be a good idea to cut out the first some generations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(de_his[10:], 'Differential Evolution (starting from the tenth gen)')\n",
    "plot(ps_his[10:], 'Particle Swarm (starting from the tenth gen)')\n",
    "plot(ga_his[10:], 'GA (generic?) (starting from the tenth gen)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to see differential evolution, the fastest method so far, go through the most generations. For all of them we see that after a certain point that we reach very quickly results improve very little. Chances are these methods could be modified to stop once improvements are not large enough (as opposed to when there has been absolutely no improvement for a while) to gain some speed.\n",
    "\n",
    "Also of note is that even if DE takes the least time to run already, it also requires a relatively small *fraction* of its runtime to reach an acceptable point, so possibly it is also the most optimizable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
