{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing a scikit model with strange ways\n",
    "\n",
    "We'll now be trying to maximize some aspect of a specific machine learning model. I've chosen for that to be accuracy.\n",
    "\n",
    "Requires the Diabetes dataset file from [Kaggle](https://www.kaggle.com/uciml/pima-indians-diabetes-database).\n",
    "\n",
    "Author: Raido Everest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('diabetes.csv')  # Just going to use these like globals. YOLO\n",
    "X = data.drop('Outcome', axis=1).values\n",
    "y = data[['Outcome']].values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common stuff and comments\n",
    "\n",
    "We've got a few more challenges here, such as discrete variables in the search space, limited range variables in the search space (errors if we'd somehow end up testing zero predictors), and a way more expensive fitness calculation than we're used to (since it is essentially necessary to train and test a model to be able to get it).\n",
    "\n",
    "I'll try to do some weird kind of discretization in the case of continuous variables to reduce search space a bit - and to possibly make memoization of results feasible. So each numeric feature will have their own min, max, and step sizes. Might actually even work okay.\n",
    "\n",
    "The parameters of the RF that we'll use are max_depth, ccp_alpha, min_impurity_decrease, so three dimensions in the search space.\n",
    "\n",
    "PS: Absolutely not as flexible as it might first look - it's still written around just this one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common to all...\n",
    "# Ranges takes care of the proper input ranges. If we leave these bounds, we project back.\n",
    "# Minimum, maximum, 'step size' in decimal points.\n",
    "ranges = [[2, 20, 0], [0.0, 1.0, 2], [0.0, 0.5, 2]]\n",
    "\n",
    "# Creates n parameter sets with given ranges for the discussed numbers.\n",
    "def create_population(n, ranges):\n",
    "    models = []\n",
    "    for _ in range(n):\n",
    "        models.append([gen_param(*dist) for dist in ranges])\n",
    "    return models\n",
    "\n",
    "# Generates a random parameter\n",
    "def gen_param(start, end, decimals):\n",
    "    rnd = round(random.random() * (end - start) + start, decimals)\n",
    "    return int(rnd) if decimals == 0 else rnd\n",
    "\n",
    "# Creates a model based on generated parameters\n",
    "def create_model(max_depth, ccp_alpha, min_impurity_decrease):\n",
    "    return RandomForestClassifier(max_depth=max_depth, ccp_alpha=ccp_alpha, min_impurity_decrease=min_impurity_decrease,\n",
    "                                 random_state=42)\n",
    "\n",
    "# Updates memory, returns the best new one it found (each method will use same kind of memory)\n",
    "# This is, of course, an awful name for the function, for its return values aren't really going to be expected\n",
    "def update_memory(memory, pop):\n",
    "    best, bestacc = None, 0.0\n",
    "    for params in pop:\n",
    "        # If model hasn't been evaluated, evaluate it.\n",
    "        a,b,c = params\n",
    "        if memory[a][b][c] == None:\n",
    "            model = create_model(a,b,c)\n",
    "            score = cross_val_score(model, X, y, cv=4).mean()\n",
    "            memory[a][b][c] = score\n",
    "            if score > bestacc:\n",
    "                best, bestacc = params, score\n",
    "    return best, bestacc\n",
    "\n",
    "# Given some next generation, fix it to be within the limited search space\n",
    "def fix_generation(pop, ranges):\n",
    "    for params in pop:\n",
    "        for i in range(len(params)):\n",
    "            # Rounds each set of parameters to be within the wanted steps\n",
    "            params[i] = round(params[i], ranges[i][2])\n",
    "            # Projects them inside if they are outside the search space\n",
    "            params[i] = max(params[i], ranges[i][0])  # Raises it to the minimum at least\n",
    "            params[i] = min(params[i], ranges[i][1])  # Lowers it to the maximum at most\n",
    "\n",
    "def plot(data, title):\n",
    "    sns.lineplot(data = data)\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Best value')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example population of model parameters\n",
    "create_population(4, ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any of these can be fed into create_model as create_model(*elem).\n",
    "create_model(*create_population(1, ranges)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differential Evolution\n",
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranges  - ranges for the model parameters\n",
    "# n       - population size\n",
    "# scaling - scaling during generating next gen\n",
    "# loops   - how many loops with no improvement to try\n",
    "def de(ranges, n=30, scaling=0.5, loops=10):\n",
    "    # Since some things are discretized now, we may be able to avoid\n",
    "    # recalculating accuracy by remembering it.\n",
    "    memory = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: None)))\n",
    "\n",
    "    # Initial model set\n",
    "    pop = create_population(n, ranges)\n",
    "    best, bestacc = update_memory(memory, pop)\n",
    "    history = [bestacc]\n",
    "    \n",
    "    # The main loop...\n",
    "    loops_since_improvement = 0\n",
    "    while loops_since_improvement < loops:\n",
    "        loops_since_improvement += 1\n",
    "        nextpop = de_next(pop, ranges, n, scaling)\n",
    "        nextbest, nextbestacc = update_memory(memory, nextpop)\n",
    "        if nextbestacc > bestacc:\n",
    "            loops_since_improvement = 0\n",
    "            best, bestacc = nextbest, nextbestacc\n",
    "        history.append(bestacc)\n",
    "        # overwrite pop with better ones from nextpop\n",
    "        de_updatepop(pop, nextpop, memory)\n",
    "\n",
    "    return best, bestacc, history\n",
    "\n",
    "# ...\n",
    "def de_next(pop, ranges, n, scaling):\n",
    "    nextpop = []\n",
    "    for i in range(n):\n",
    "        a, b = random.choice(pop), random.choice(pop)  # Picking two random elements\n",
    "        nextpop.append([pop[i][j]+scaling*(b[j]-a[j]) for j in range(len(a))])  # Add the scaled ab vector to pop[i]...\n",
    "        \n",
    "    fix_generation(nextpop, ranges)  # Solve some problems\n",
    "    return nextpop\n",
    "\n",
    "# ...\n",
    "def de_updatepop(pop, nextpop, memory):\n",
    "    # Now that everything in pop and nextpop are guaranteed to be in memory,\n",
    "    # this couldn't be easier.\n",
    "    for i in range(len(pop)):\n",
    "        a1,b1,c1 = pop[i]\n",
    "        a2,b2,c2 = nextpop[i]\n",
    "        if memory[a2][b2][c2] > memory[a1][b1][c1]:\n",
    "            pop[i] = nextpop[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check\n",
    "\n",
    "Seems to do okay and get a considered-good accuracy in the end, however the scaling has to be quite high (even above one for best outcome speed wise) for it to really go to a good place (and to be truly performant - by smashing into the corners of some variable range it's a lot more likely to try to compute something we've already computed). Feels strange to do it like this, I'll test it with several configs later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best, bestval, his = de(ranges, 15, 1.0, 10)\n",
    "(best, bestval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy graph over time\n",
    "\n",
    "We find we get the best model VERY early though - in one or two generations it won't move any more. May be because of the chosen parameters, may be the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(his, \"Differential Evolution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuring time\n",
    "\n",
    "So we know we spend most of the time for no reason. So let's measure and contrast the time this method takes with waiting ~3 loops for improvement and ~10 like above to see if we check many models we haven't made yet. (Also freeze the seed because otherwise things may get weird)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "%timeit -r 1 -n 1 de(ranges, 15, 1.0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "%timeit -r 1 -n 1 de(ranges, 15, 1.0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't seem like adding seven extra loops changed runtime so much, suggesting we do get a lot of collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "%timeit -r 1 -n 1 de(ranges, 15, 3.0, 3)  # Very high scaling - makes big jumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigger jumps seem to have cut down on processing time some."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Particle Swarm Optimization\n",
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSO: still broken as ever.\n",
    "# ranges  - ranges for the model parameters\n",
    "# n       - population size\n",
    "# loops   - how many loops with no improvement to try\n",
    "# lr      - how much velocity affects future velocity (between 0 and 1)\n",
    "# c1 and c2   - weighting for personal and overall best when moving  (between 0 and 1)\n",
    "# res_speed   - the speed we may reset to randomly\n",
    "# res_speed_p - probability of reset\n",
    "def pso(ranges, n=15, loops=5, lr=1.0, c1=0.5, c2=0.5, res_speed=5, res_speed_p=0.1):\n",
    "    # Creating memory in hopes that it helps us catch potential collisions in the discretized search space\n",
    "    memory = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: None)))\n",
    "\n",
    "    # Initial model set\n",
    "    pop     = create_population(n, ranges)\n",
    "    popbest = pop[::]\n",
    "    velocity      = pso_init_velocities(n)\n",
    "    best, bestacc = update_memory(memory, pop)\n",
    "    history = [bestacc]\n",
    "    \n",
    "    # The main loop...\n",
    "    loops_since_improvement = 0\n",
    "    while loops_since_improvement < loops:\n",
    "        loops_since_improvement += 1\n",
    "        \n",
    "        pso_iterate(ranges, velocity, pop, popbest, best, lr, c1, c2, res_speed, res_speed_p)\n",
    "        nextbest, nextbestacc = update_memory(memory, pop)\n",
    "        pso_update_personal_best(memory, pop, popbest)\n",
    "        if nextbestacc > bestacc:\n",
    "            loops_since_improvement = 0\n",
    "            best, bestacc = nextbest, nextbestacc\n",
    "        history.append(bestacc)\n",
    "\n",
    "    return best, bestacc, history\n",
    "\n",
    "# ...\n",
    "def pso_init_velocities(n):\n",
    "    choices = list(range(-2,3))\n",
    "    velocities = []\n",
    "    for _ in range(n):\n",
    "        velocities.append([random.choice(choices) for _ in range(3)])\n",
    "    return velocities\n",
    "\n",
    "# ...\n",
    "def normalise(vector, length=1):\n",
    "    l = np.dot(vector, vector)**0.5\n",
    "    return [v/l for v in vector]\n",
    "\n",
    "# ...\n",
    "# Mutates pop and velocity\n",
    "def pso_iterate(ranges, velocity, pop, popbest, best, lr, c1, c2, res_speed, res_speed_p):\n",
    "    for i in range(len(pop)):\n",
    "        z1, z2 = random.random(), random.random()\n",
    "        velocity[i] = list(np.add(lr * np.array(velocity[i]),\n",
    "                                  np.add(c1*z1*np.subtract(popbest[i], pop[i]), c2*z2*np.subtract(best, pop[i]))))\n",
    "        pop[i] = list(np.add(pop[i], velocity[i]))\n",
    "        # Reset speed for each particle individually this time around.\n",
    "        if random.random() < res_speed_p:\n",
    "            velocity[i] = normalise(velocity[i], res_speed)\n",
    "    # Now we have to fix up any particles that have left the search space\n",
    "    fix_generation(pop, ranges)\n",
    "\n",
    "# ...\n",
    "# Mutates popbest\n",
    "def pso_update_personal_best(memory, pop, popbest):\n",
    "    for i in range(len(pop)):\n",
    "        a1,b1,c1 = pop[i]\n",
    "        a2,b2,c2 = popbest[i]\n",
    "        if memory[a1][b1][c1] > memory[a2][b2][c2]:\n",
    "            popbest[i] = pop[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check\n",
    "\n",
    "Finds a pretty good solution as well. What really matters now is the speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best, bestval, his = pso(ranges)\n",
    "(best, bestval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy graph over time\n",
    "\n",
    "Same 'issue' of converging very quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(his, \"Particle Swarm Optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuring time\n",
    "\n",
    "Doesn't seem to be too impressive, even given the kinda low pop we start with. If we cut pop, will it converge quicker?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "%timeit -r 1 -n 1 pso(ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "%timeit -r 1 -n 1 print(max(pso(ranges, n=10)[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we beat DE time with similar end result... What if we go a lot further again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "%timeit -r 1 -n 1 print(max(pso(ranges, n=5)[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's starting to hurt a tiny bit, but the speed is real. Let's try DE once more, trying to do as little work as possible..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "%timeit -r 1 -n 1 print(max(de(ranges, 12, 6.0, 3)[2]))  # Super big jumps and smaller pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same time for same result. This definitely makes life hard for those of us that just want to call something the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genetic Algorithm\n",
    "\n",
    "Finally, it is time for something simple.\n",
    "\n",
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranges  - ranges for the model parameters\n",
    "# n       - population size\n",
    "# bign    - candidates popultion size\n",
    "# loops   - how many loops with no improvement to try\n",
    "def ga(ranges, n=20, bign=60, loops=5):\n",
    "    # Creating memory in hopes that it helps us catch potential collisions in the discretized search space\n",
    "    memory = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: None)))\n",
    "\n",
    "    # Initial model set\n",
    "    pop     = create_population(n, ranges)\n",
    "    best, bestacc = update_memory(memory, pop)\n",
    "    history = [bestacc]\n",
    "    \n",
    "    # The main loop...\n",
    "    loops_since_improvement = 0\n",
    "    while loops_since_improvement < loops:\n",
    "        loops_since_improvement += 1\n",
    "        \n",
    "        bigpop = ga_nextpop(ranges, pop, bign)\n",
    "        nextbest, nextbestacc = update_memory(memory, bigpop)\n",
    "        if nextbestacc > bestacc:\n",
    "            loops_since_improvement = 0\n",
    "            best, bestacc = nextbest, nextbestacc\n",
    "        \n",
    "        # Sort the models based on their accuracy and take only the best ones.\n",
    "        pop = sorted(pop + bigpop, key=lambda x: memory[x[0]][x[1]][x[2]], reverse=True)[:n]\n",
    "        \n",
    "        history.append(bestacc)\n",
    "\n",
    "    return best, bestacc, history\n",
    "\n",
    "# ...\n",
    "def ga_nextpop(ranges, pop, bign):\n",
    "    nextpop = []\n",
    "    for _ in range(bign):\n",
    "        x1, x2 = random.choice(pop), random.choice(pop)\n",
    "        nextpop.append(mutate(crossover(x1, x2)))\n",
    "    fix_generation(nextpop, ranges)  # Solve out of range/legal search space issues\n",
    "    return nextpop\n",
    "\n",
    "# ...\n",
    "def mutate(vector):\n",
    "    if random.random() < 0.25:\n",
    "        return vector  # just don't bother with some\n",
    "    newvector = [elem * (random.random() * 1.5 + 0.5) for elem in vector]\n",
    "    return newvector\n",
    "\n",
    "# ...\n",
    "def crossover(x1, x2):\n",
    "    return [x1[i] if random.random() < 0.5 else x2[i] for i in range(len(x1))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check\n",
    "\n",
    "0.66 might be the worst result so far and it wasn't exactly quick to arrive either..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best, bestval, his = ga(ranges, n=10, bign=15)\n",
    "(best, bestval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy graph over time\n",
    "\n",
    "But yeah, still does the same thing. Gets results fast and stays there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(his, \"Genetic Algorithm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuring time\n",
    "\n",
    "Long story short, not better for more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "%timeit -r 1 -n 1 ga(ranges, n=10, bign=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just run away from GA now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "Differential evolution and PSO are just doing a great job, at least given this dataset. The generic genetic algorithm unfortunately suffers from performance issues, and in the end finds a result with quite a bad accuracy, I imagine due to the way it ended up going through the search space (it is the only method that gave nonzero float parameters for example and it is kind of biased toward higher values since mutation scales uniformly from 0.5x to 2x)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
