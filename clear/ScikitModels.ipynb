{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing a scikit model with strange ways\n",
    "\n",
    "We'll now be trying to maximize some aspect of a specific machine learning model. I've chosen for that to be accuracy.\n",
    "\n",
    "Requires the Diabetes dataset file from [Kaggle](https://www.kaggle.com/uciml/pima-indians-diabetes-database).\n",
    "\n",
    "Author: Raido Everest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('diabetes.csv')  # Just going to use these like globals. YOLO\n",
    "X = data.drop('Outcome', axis=1).values\n",
    "y = data[['Outcome']].values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common stuff and comments\n",
    "\n",
    "We've got a few more challenges here, such as discrete variables in the search space, limited range variables in the search space (errors if we'd somehow end up testing zero predictors), and a way more expensive fitness calculation than we're used to (since it is essentially necessary to train and test a model to be able to get it).\n",
    "\n",
    "I'll try to do some weird kind of discretization in the case of continuous variables to reduce search space a bit - and to possibly make memoization of results feasible. So each numeric feature will have their own min, max, and step sizes. Might actually even work okay.\n",
    "\n",
    "The parameters of the RF that we'll use are max_depth, ccp_alpha, min_impurity_decrease, so three dimensions in the search space.\n",
    "\n",
    "PS: Absolutely not as flexible as it might first look - it's still written around just this one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common to all...\n",
    "# Ranges takes care of the proper input ranges. If we leave these bounds, we project back.\n",
    "# Minimum, maximum, 'step size' in decimal points.\n",
    "ranges = [[2, 20, 0], [0.0, 1.0, 2], [0.0, 0.5, 2]]\n",
    "\n",
    "# Creates n parameter sets with given ranges for the discussed numbers.\n",
    "def create_population(n, ranges):\n",
    "    models = []\n",
    "    for _ in range(n):\n",
    "        models.append([gen_param(*dist) for dist in ranges])\n",
    "    return models\n",
    "\n",
    "# Generates a random parameter\n",
    "def gen_param(start, end, decimals):\n",
    "    rnd = round(random.random() * (end - start) + start, decimals)\n",
    "    return int(rnd) if decimals == 0 else rnd\n",
    "\n",
    "# Creates a model based on generated parameters\n",
    "def create_model(max_depth, ccp_alpha, min_impurity_decrease):\n",
    "    return RandomForestClassifier(max_depth=max_depth, ccp_alpha=ccp_alpha, min_impurity_decrease=min_impurity_decrease,\n",
    "                                 random_state=42)\n",
    "\n",
    "# Updates memory, returns the best new one it found (each method will use same kind of memory)\n",
    "# This is, of course, an awful name for the function, for its return values aren't really going to be expected\n",
    "def update_memory(memory, pop):\n",
    "    best, bestacc = None, 0.0\n",
    "    for params in pop:\n",
    "        # If model hasn't been evaluated, evaluate it.\n",
    "        a,b,c = params\n",
    "        if memory[a][b][c] == None:\n",
    "            model = create_model(a,b,c)\n",
    "            score = cross_val_score(model, X, y, cv=4).mean()\n",
    "            memory[a][b][c] = score\n",
    "            if score > bestacc:\n",
    "                best, bestacc = params, score\n",
    "    return best, bestacc\n",
    "\n",
    "# Given some next generation, fix it to be within the limited search space\n",
    "def fix_generation(pop, ranges):\n",
    "    for params in pop:\n",
    "        for i in range(len(params)):\n",
    "            # Rounds each set of parameters to be within the wanted steps\n",
    "            params[i] = round(params[i], ranges[i][2])\n",
    "            # Projects them inside if they are outside the search space\n",
    "            params[i] = max(params[i], ranges[i][0])  # Raises it to the minimum at least\n",
    "            params[i] = min(params[i], ranges[i][1])  # Lowers it to the maximum at most\n",
    "\n",
    "def plot(data, title):\n",
    "    sns.lineplot(data = data)\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Best value')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example population of model parameters\n",
    "create_population(4, ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any of these can be fed into create_model as create_model(*elem).\n",
    "create_model(*create_population(1, ranges)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differential Evolution\n",
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranges  - ranges for the model parameters\n",
    "# n       - population size\n",
    "# scaling - scaling during generating next gen\n",
    "# loops   - how many loops with no improvement to try\n",
    "def de(ranges, n=30, scaling=0.5, loops=20):\n",
    "    # Since some things are discretized now, we may be able to avoid\n",
    "    # recalculating accuracy by remembering it.\n",
    "    memory = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: None)))\n",
    "\n",
    "    # Initial model set\n",
    "    pop = create_population(n, ranges)\n",
    "    best, bestacc = update_memory(memory, pop)\n",
    "    history = [bestacc]\n",
    "    \n",
    "    # The main loop...\n",
    "    loops_since_improvement = 0\n",
    "    while loops_since_improvement < loops:\n",
    "        loops_since_improvement += 1\n",
    "        nextpop = de_next(pop, ranges, n, scaling)\n",
    "        nextbest, nextbestacc = update_memory(memory, nextpop)\n",
    "        if nextbestacc > bestacc:\n",
    "            loops_since_improvement = 0\n",
    "            best, bestacc = nextbest, nextbestacc\n",
    "        history.append(bestacc)\n",
    "        # overwrite pop with better ones from nextpop\n",
    "        de_updatepop(pop, nextpop, memory)\n",
    "\n",
    "    return best, bestacc, history\n",
    "\n",
    "# ...\n",
    "def de_next(pop, ranges, n, scaling):\n",
    "    nextpop = []\n",
    "    for i in range(n):\n",
    "        a, b = random.choice(pop), random.choice(pop)  # Picking two random elements\n",
    "        nextpop.append([pop[i][j]+scaling*(b[j]-a[j]) for j in range(len(a))])  # Add the scaled ab vector to pop[i]...\n",
    "        \n",
    "    fix_generation(nextpop, ranges)  # Solve some problems\n",
    "    return nextpop\n",
    "\n",
    "# ...\n",
    "def de_updatepop(pop, nextpop, memory):\n",
    "    # Now that everything in pop and nextpop are guaranteed to be in memory,\n",
    "    # this couldn't be easier.\n",
    "    for i in range(len(pop)):\n",
    "        a1,b1,c1 = pop[i]\n",
    "        a2,b2,c2 = nextpop[i]\n",
    "        if memory[a2][b2][c2] > memory[a1][b1][c1]:\n",
    "            pop[i] = nextpop[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check\n",
    "\n",
    "Seems to do okay and get a considered-good accuracy in the end, however the scaling has to be quite high (even above one for best outcome speed wise) for it to really go to a good place (and to be truly performant - by smashing into the corners of some variable range it's a lot more likely to try to compute something we've already computed). Feels strange to do it like this, I'll test it with several configs later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best, bestval, his = de(ranges, 15, 1.0, 10)\n",
    "(best, bestval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy graph over time\n",
    "\n",
    "We find we get the best model VERY early though - in one or two generations it won't move any more. May be because of the chosen parameters, may be the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(his, \"Differential Evolution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuring time\n",
    "\n",
    "So we know we spend most of the time for no reason. So let's measure and contrast the time this method takes with waiting ~3 loops for improvement and ~10 like above to see if we check many models we haven't made yet. (Also freeze the seed because otherwise things may get weird)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "%timeit -r 1 -n 1 de(ranges, 15, 1.0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "%timeit -r 1 -n 1 de(ranges, 15, 1.0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't seem like adding seven extra loops changed runtime so much, suggesting we do get a lot of collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "%timeit -r 1 -n 1 de(ranges, 15, 3.0, 3)  # Very high scaling - makes big jumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigger jumps seem to have cut down on processing time some."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
